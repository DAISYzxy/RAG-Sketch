{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "task_type = \"clustering\"\n",
    "file_name = \"qwen_answer_\" + task_type + \"_set4.json\"\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    final_answer = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_spotlight = pd.read_csv(\"df_new_clustering.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "for idx in range(len(df_new_spotlight)):\n",
    "    df_new_spotlight[\"doc\"][idx] = ast.literal_eval(df_new_spotlight[\"doc\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse for the legal answer\n",
    "import re\n",
    "def extract_numbers(text):\n",
    "    \"\"\"提取文本中的所有数字\"\"\"\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    return [int(num) for num in numbers]\n",
    "\n",
    "if task_type == \"spotlight\" or task_type == \"comparison\":\n",
    "    for idx in range(len(df_new_spotlight)):\n",
    "        type_query = df_new_spotlight[\"type\"][idx]\n",
    "        if type_query == \"legal\":\n",
    "            answer = df_new_spotlight[\"answer\"][idx]\n",
    "            doc_lst = df_new_spotlight[\"doc\"][idx]\n",
    "            num_lst = extract_numbers(answer)\n",
    "            answer_lst = []\n",
    "            for num in num_lst:\n",
    "                answer_lst.append(doc_lst[num-1])\n",
    "            df_new_spotlight[\"answer\"][idx] = answer_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'We would like to request your feedback on the performance of the AI assistant in response to the user question displayed above according to the gold answer. Please use the following listed aspects and their descriptions as evaluation criteria: \\n - Accuracy and Hallucinations: The assistant’s answer is semantically consistent with the gold answer; The numerical value and order need to be accurate, and there should be no hallucinations. \\n- Completeness: Referring to the reference answers, the assistant’s answer should contain all the key points needed to answer the user’s question; further elaboration on these key points can be omitted. Please rate whether this answer is suitable for the question. Please note that the gold answer can be considered as a correct answer to the question. The assistant receives an overall score on a scale of 1 to 100, where a higher score indicates better overall performance.Please note that if the assistant’s answer and the gold answer fully meet the above criteria, its overall rating should be the full marks (100). Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias.Then, output a line indicating the score of the Assistant. PLEASE OUTPUT WITH THE FOLLOWING FORMAT, WHERE THE SCORE IS A SCALE OF 1 TO 100 BY STRICTLY FOLLOWING THIS FORMAT: \"[[score]]\", FOR EXAMPLE \"Rating: [[100]]\": \\n <Start Output>\\nEvaluation evidence: your evluation explanation here, no more than 100 words Rating: [[score]]\\n<End Output>\\nNow, start your evaluation:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_gpt_response(query):\n",
    "    \"\"\"发送最终提问，并获取 GPT 的回复\"\"\"\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": query})  # 加入提问\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"messages\": conversation_history,  # 传递完整的对话历史\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        response_json = response.json()\n",
    "        \n",
    "        if \"choices\" in response_json:\n",
    "            answer = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "            print(\"> Answer: \", answer)\n",
    "            return answer\n",
    "        else:\n",
    "            print(\"⚠️ API 响应异常:\", response_json)\n",
    "            return \"API_ERROR\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ 请求异常: {e}\")\n",
    "        return \"REQUEST_ERROR\"\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠️ JSON 解析错误\")\n",
    "        return \"JSON_ERROR\"\n",
    "\n",
    "\n",
    "\n",
    "url = \"\"\n",
    "headers = { \n",
    "    \"Content-Type\": \"application/json\", \n",
    "    \"Authorization\": \"\"\n",
    "}\n",
    "\n",
    "# 存储对话历史\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手。\"}  # 设定初始角色\n",
    "]\n",
    "\n",
    "def add_context(text):\n",
    "    \"\"\"向 GPT 添加多段长文本作为上下文\"\"\"\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_spotlight_set1 = df_new_spotlight[df_new_spotlight[\"set\"] == 4].reset_index(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 88)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_answer), len(df_new_spotlight_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Answer:  <Start Output>\n",
      "Evaluation evidence: The assistant's answer is incomplete and inaccurate compared to the gold answer. It fails to list any references, whereas the gold answer includes five references. Additionally, the assistant's citation list is incorrect, missing two citations from the gold answer and including two that are not present in the gold answer. The assistant's response also contains hallucinations, as it lists citations that are not in the gold answer. Therefore, the assistant's response lacks accuracy and completeness. Rating: [[30]]\n",
      "<End Output>\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "for idx in range(0, len(df_new_spotlight_set1)):\n",
    "    conversation_history.clear()  # 清空对话历史\n",
    "    question = df_new_spotlight_set1[\"question\"][idx]\n",
    "    gold_answer = str(df_new_spotlight_set1[\"answer\"][idx])\n",
    "    type = df_new_spotlight_set1[\"type\"][idx]\n",
    "    llm_predict = final_answer[idx]\n",
    "    add_context(\"[The given question]: \" + question)\n",
    "    if type == \"legal\":\n",
    "        if task_type == \"spotlight\" or task_type == \"comparison\":\n",
    "            add_context(\"[Gold Answer]: \" + gold_answer)\n",
    "        else:\n",
    "            add_context(\"[Gold Answer]: \" + gold_answer + \" 其中序号对应的判决书列表如下： \" + str(df_new_spotlight_set1[\"doc\"][idx]))\n",
    "    else:\n",
    "        add_context(\"[Gold Answer]: \" + gold_answer)\n",
    "    add_context(\"[The start of Assistant's predicted Answer]\\n\" + llm_predict + \"[The End of Assistant’s Predicted Answer]\")\n",
    "    response = get_gpt_response(instruction)\n",
    "    score.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_num = []\n",
    "for output in score:\n",
    "    score_num.append(extract_numbers(output)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst):\n",
    "    return sum(lst) / len(lst) if lst else 0  # 避免除零错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.7132867132867"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average(score_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.47%\n"
     ]
    }
   ],
   "source": [
    "def calculate_percentage(lst):\n",
    "    count_100 = lst.count(100)\n",
    "    total = len(lst)\n",
    "    percentage = (count_100 / total) * 100 if total > 0 else 0\n",
    "    return f\"{percentage:.2f}%\"\n",
    "\n",
    "print(calculate_percentage(score_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
